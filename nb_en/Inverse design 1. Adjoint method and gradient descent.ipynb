{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3ba2b9",
   "metadata": {},
   "source": [
    "MIT Licence\n",
    "\n",
    "© Alexey A. Shcherbakov, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffec2d",
   "metadata": {},
   "source": [
    "# Lecture 5.1. Adjoint method and gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab869914",
   "metadata": {},
   "source": [
    "Currently, inverse and optimization problems in English-language literature are called methods of reverse engineering. The solution of such problems occurs iteratively, and therefore must include a mathematical model and an appropriate numerical method for solving the direct problem, as well as an optimization algorithm that allows you to come to the desired parameters of the problem in accordance with the target performance of the device in question specified by the user.\n",
    "\n",
    "The mathematical formulation of the optimization problem involves the consideration of the so-called objective function, for which it is necessary to find an extremum on a certain set of parameters. The optimization problem may have only one or several optima. In this latter case, one of the solutions is the global optimum, and the others are local. The global optimum is the best solution to the optimization problem, while the local optimum will have a better value of the objective function than the points in its neighborhood, but worse than the global optimum. Thus, global optimization deals with the goal of finding the best global solution to problems with non-convex objective functions, which is known as a multimodal optimization problem. The methods proposed for solving global optimization problems can be divided into two main groups: deterministic and stochastic methods.\n",
    "\n",
    "Methods for solving inverse problems can be divided into methods that use information about the gradient of the objective function and those that do not. Gradient optimization is the most widely used class of deterministic methods for solving inverse problems of photonics. It is an iterative algorithm in which the gradient vector of the objective function is calculated at each iteration with respect to all parameters of the problem, and then these parameters are changed in the direction of the gradient. If there are many parameters and the direct method is quite labor-intensive, then calculating partial derivatives with respect to all parameters through finite differences requires excessive computational costs. To solve this problem, the so-called adjoint method is used, which requires solving only two direct problems to calculate the full gradient vector regardless of the number of parameters.\n",
    "\n",
    "This lecture provides a brief overview of gradient methods and presents formulations of the adjoint method for linear and nonlinear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1a395",
   "metadata": {},
   "source": [
    "## Gradient evaluation with the Adjoint Method)\n",
    "\n",
    "Let us consider a linear problem. Let the wave equations be solved by a numerical method that reduces to a system of linear algebraic equations:\n",
    "\\begin{equation}\n",
    "    M \\boldsymbol{a} = \\boldsymbol{a}_{inc}\n",
    "\\end{equation}\n",
    "Let us denote the vector of optimization parameters as $\\boldsymbol{p}$, and the objective function as $\\mathcal{F}=\\mathcal{F}(\\boldsymbol{a}(\\boldsymbol{p}))$. We will assume that the objective function is an analytically known function of the vector $\\boldsymbol{a}$, so that the derivatives $\\partial\\mathcal{F}/\\partial\\boldsymbol{a}$ are easily computable. Let us consider the partial derivative of the objective functions with respect to one of the parameters:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{F}}{\\partial p_j} = \\frac{\\partial\\mathcal{F}}{\\partial\\boldsymbol{a}} \\frac{\\partial\\boldsymbol{a}}{\\partial p_j}\n",
    "\\end{equation}\n",
    "To calculate the last factor, let us consider the original direct problem and differentiate it with respect to the parameter under consideration:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial M}{\\partial p_j} \\boldsymbol{a} + M \\frac{\\partial\\boldsymbol{a}}{\\partial p_j} = \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} \\; \\Rightarrow \\; \\frac{\\partial\\boldsymbol{a}}{\\partial p_j} = M^{-1} \\left( \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} - \\frac{\\partial M}{\\partial p_j} \\boldsymbol{a} \\right)\n",
    "\\end{equation}\n",
    "Then\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{F}}{\\partial p_j} = \\left( \\frac{\\partial\\mathcal{F}}{\\partial\\boldsymbol{a}} M^{-1} \\right) \\left( \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} - \\frac{\\partial M}{\\partial p_j} \\boldsymbol{a} \\right) = \\boldsymbol{a}^T_{adj} \\left( \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} - \\frac{\\partial M}{\\partial p_j} \\boldsymbol{a} \\right)\n",
    "\\end{equation}\n",
    "Here $\\boldsymbol{a}_{adj}$ is the solution of the adjoint problem\n",
    "\\begin{equation}\n",
    "    M^T \\boldsymbol{a}_{adj} = \\left( \\frac{\\partial\\mathcal{F}}{\\partial\\boldsymbol{a}} \\right)^T\n",
    "\\end{equation}\n",
    "Зачастую, наиболее вычислительно сложной частью оптмизационного процесса является решение прямой задачи, и метод сопряжения позволяет вычислять вектор градиента по заданному вектору параметров через решение всего двух прямых задач.\n",
    "\n",
    "In a more general case of a nonlinear problem, the solution satisfies some equation\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{\\varphi} \\left( \\boldsymbol{a}, \\boldsymbol{a}_{inc}, \\boldsymbol{p} \\right) = 0\n",
    "\\end{equation}\n",
    "Its differentiation gives\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}} \\frac{\\partial\\boldsymbol{a}}{\\partial p_j} + \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}_{inc}} \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} + \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial p_j} = 0 \\Rightarrow\n",
    "    \\frac{\\partial\\boldsymbol{a}}{\\partial p_j} = -\\left( \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}} \\right)^{-1} \\left( \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}_{inc}} \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} + \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial p_j} \\right)\n",
    "\\end{equation}\n",
    "Then for the gradient component with respect to the parameter $p_j$ we have\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{F}}{\\partial p_j} = \n",
    "    \\frac{\\partial\\mathcal{F}}{\\partial\\boldsymbol{a}} \\frac{\\partial\\boldsymbol{a}}{\\partial p_j} = \n",
    "    -\\left[ \\frac{\\partial\\mathcal{F}}{\\partial\\boldsymbol{a}} \\left( \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}} \\right)^{-1} \\right] \\left( \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}_{inc}} \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} + \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial p_j} \\right) =\n",
    "    - \\boldsymbol{\\chi} \\left( \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial\\boldsymbol{a}_{inc}} \\frac{\\partial\\boldsymbol{a}_{inc}}{\\partial p_j} + \\frac{\\partial\\boldsymbol{\\varphi}}{\\partial p_j} \\right)\n",
    "\\end{equation}\n",
    "The main difference from the linear case is that the equation on $\\boldsymbol{\\chi}$ is not conjugate to the original nonlinear equation, but the complexity of its solution is usually less than the complexity of the original one, since it is reduced to multiplying the inverse matrix by a given vector.\n",
    "\n",
    "As an example, consider the problem of optimizing the funcional $\\mathcal{F}(\\boldsymbol{a},\\lambda,\\boldsymbol{p})$ for the eigenvalue problem\n",
    "\\begin{equation}\n",
    "    M \\boldsymbol{a} = \\lambda \\boldsymbol{a}\n",
    "\\end{equation}\n",
    "For simplicity, we will assume that the matrix of the system is real symmetric, and the eigenvalues ​​are non-degenerate. To reduce the problem to a nonlinear one, we consider the vector\n",
    "\\begin{equation}\n",
    "    \\tilde{\\boldsymbol{a}} = \\left( \\begin{array}{c} \\boldsymbol{a} \\\\ \\lambda \\end{array} \\right)\n",
    "\\end{equation}\n",
    "The equality to zero $M \\boldsymbol{a} - \\lambda \\boldsymbol{a} = 0$ must be supplemented by one more condition, since the number of parameters has increased by 1. Let it be $\\boldsymbol{a}^T\\boldsymbol{a}=1$, then\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{\\varphi} = \\left( \\begin{array}{c} M \\boldsymbol{a} - \\lambda \\boldsymbol{a} \\\\ \\boldsymbol{a}^T\\boldsymbol{a}-1 \\end{array} \\right)\n",
    "\\end{equation}\n",
    "We write the desired solution to the adjoint problem as $(\\boldsymbol{a}_{adj}; \\alpha)^T$. Then the adjoint equation will take the form\n",
    "\\begin{align}\n",
    "    (M - \\lambda\\mathbb{I}) \\boldsymbol{a}_{adj} &= \\left( \\frac{\\partial\\mathcal{F}}{\\partial\\boldsymbol{a}} \\right)^T - 2\\alpha\\boldsymbol{a} \\\\\n",
    "    -\\boldsymbol{a}^T \\boldsymbol{a}_{adj} &= \\frac{\\partial\\mathcal{F}}{\\partial\\lambda}\n",
    "\\end{align}\n",
    "Since the matrix $(M - \\lambda\\mathbb{I})$, we choose $\\alpha$ such that the first equation has a solution: $\\boldsymbol{a}^T (\\mathcal{F}_{\\boldsymbol{a}}^T - 2\\alpha\\boldsymbol{a}) = 0$, whence $\\alpha = (1/2)\\boldsymbol{a}^T \\mathcal{F}_{\\boldsymbol{a}}^T$. Then the solutions of the resulting equation\n",
    "\\begin{equation}\n",
    "    (M - \\lambda\\mathbb{I}) \\boldsymbol{a}_{adj} = (1-\\boldsymbol{a}\\boldsymbol{a}^T) \\mathcal{F}_{\\boldsymbol{a}}^T = P \\mathcal{F}_{\\boldsymbol{a}}^T\n",
    "\\end{equation}\n",
    "we write it in the form $\\boldsymbol{a}_{adj} = \\boldsymbol{a}_{adj0} + \\gamma \\boldsymbol{a}$, where $\\boldsymbol{a}^T\\boldsymbol{a}_{adj0} = 0$. As a result, we obtain the gradient\n",
    "\\begin{equation}\n",
    "    \\left. \\frac{d\\mathcal{F}}{d\\boldsymbol{p}} \\right|_{\\boldsymbol{\\varphi}=0} = \\mathcal{F}_{\\boldsymbol{p}} - \\boldsymbol{a}_{adj0}^T M_p \\boldsymbol{a} + \\mathcal{F}_{\\lambda} \\boldsymbol{a}^T A_p \\boldsymbol{a}\n",
    "\\end{equation}\n",
    "The last term of the equation expresses the well-known result in quantum physics, the Hellman-Feynman theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f555c7",
   "metadata": {},
   "source": [
    "## First-order method\n",
    "\n",
    "Now, having at our disposal an algorithm for efficient numerical calculation of gradients, let us consider the most widely used gradient methods for solving optimization problems.\n",
    "\n",
    "### Steepest Descent Algorithm\n",
    "\n",
    "As above, consider the objective function $\\mathcal{F}=\\mathcal{F}(\\boldsymbol{a}(\\boldsymbol{p}))$. The iteration formula for the steepest descent algorithm is\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{p}_{k+1} = \\boldsymbol{p}_{k} + \\lambda_k \\nabla\\mathcal{F}(\\boldsymbol{p}_{k})/||\\nabla\\mathcal{F}(\\boldsymbol{p}_{k})||\n",
    "\\end{equation}\n",
    "in this case, the coefficient is selected based on the solution of the one-dimensional optimization problem \n",
    "\\begin{equation}\n",
    "    \\lambda_k = \\mathrm{arg}\\min_{\\lambda} \\mathcal{F}(\\boldsymbol{p}_{k} + \\lambda \\boldsymbol{S}_k)\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{S}_k = \\nabla\\mathcal{F}(\\boldsymbol{p}_{k})/||\\nabla\\mathcal{F}(\\boldsymbol{p}_{k})||$ is the normalized gradient of the objective function - the direction of searching for a new approximation. In this formulation, the gradient at a new point is orthogonal to the direction of the previous descent step. The algorithm converges quickly away from an extremum and slowly near such a point.\n",
    "\n",
    "### Conjugate gradient method\n",
    "\n",
    "Unlike the steepest descent algorithm, the conjugate gradient method uses information about the derivatives of the function at previous steps. The search direction at the current iteration is selected as a linear combination of the gradient at this step and the search directions at previous steps, and the coefficients in the combination are chosen so as to make the directions conjugate with respect to the Hessian.\n",
    "\n",
    "At the first step $\\boldsymbol{p}_{1} = \\boldsymbol{p}_{0} - \\lambda_0 \\nabla\\mathcal{F}(\\boldsymbol{p}_{0}) = \\boldsymbol{p}_{0} + \\lambda_0 \\boldsymbol{S}_{0}$, where the coefficient is chosen based on the condition $\\lambda_0 = \\mathrm{arg}\\min_{\\lambda}(\\mathcal{F}(\\boldsymbol{p}_{0})+\\lambda\\boldsymbol{S}_{0})$. The direction of descent at the second step is chosen as a linear combination of $\\boldsymbol{S}_{1} = -\\nabla\\mathcal{F}(\\boldsymbol{p}_{1}) + \\beta_1 \\boldsymbol{S}_{0}$ such that the above conjugacy condition $\\boldsymbol{S}_{0}^T H \\boldsymbol{S}_{1} = 0$ is satisfied. It can be shown that this condition leads to the following form of the coefficient:\n",
    "\\begin{equation}\n",
    "    \\beta_k = -\\frac{||\\nabla\\mathcal{F}(\\boldsymbol{p}_{k})||}{||\\nabla\\mathcal{F}(\\boldsymbol{p}_{k-1})||}\n",
    "\\end{equation}\n",
    "In this formulation, the method is called the Fletcher-Reeves method. In another common formulation, Polak-Ribiere, the coefficient looks like this:\n",
    "\\begin{equation}\n",
    "    \\beta_k = -\\frac{\\nabla\\mathcal{F}(\\boldsymbol{p}_{k}) \\cdot \\left[ \\nabla\\mathcal{F}(\\boldsymbol{p}_{k}) - \\nabla\\mathcal{F}(\\boldsymbol{p}_{k-1}) \\right]}{\\nabla\\mathcal{F}(\\boldsymbol{p}_{k-1}) \\cdot \\boldsymbol{p}_{k-1}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7775b",
   "metadata": {},
   "source": [
    "## Second-order methods\n",
    "\n",
    "Second-order methods are based on the expansion of a function of the form\n",
    "\\begin{equation}\n",
    "    \\mathcal{F}(\\boldsymbol{p}+\\Delta\\boldsymbol{p}) \\approx \\mathcal{F}(\\boldsymbol{p}) + \\nabla \\mathcal{F}(\\boldsymbol{p}) \\cdot \\Delta\\boldsymbol{p} + \\frac{1}{2} \\Delta\\boldsymbol{p} \\cdot H(\\boldsymbol{p}) \\Delta\\boldsymbol{p}\n",
    "\\end{equation}\n",
    "One of the most common algorithms is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. The laborious calculation of the Hessian is replaced here by calculating the approximate value of the corresponding inverse matrix at each step of the method $\\tilde{G}_k \\approx H^{-1}$, so that the minimum of the quadratic problem is $\\Delta\\boldsymbol{p}_k = -\\tilde{G}_k\\nabla\\mathcal{F}(\\boldsymbol{p}_k)$\n",
    "In this case, the step of updating the approximate inverse Hessian\n",
    "\\begin{equation}\n",
    "    \\tilde{G}_{k+1} = \\left[ \\mathbb{I} - \\rho_k\\Delta\\boldsymbol{p}_k \\Delta(\\nabla\\mathcal{F})_k^T \\right] \\tilde{G}_{k} \\left[ \\mathbb{I} - \\rho_k \\Delta(\\nabla\\mathcal{F})_k \\Delta\\boldsymbol{p}_k^T \\right] + \\rho_k \\Delta\\boldsymbol{p}_k \\Delta\\boldsymbol{p}_k^T\n",
    "\\end{equation}\n",
    "and as an initial approximation of the inverse Hessian one can take the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b96435",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "1. S. G. Johnson, [Notes on Adjoint Methods](https://math.mit.edu/~stevenj/18.336/adjoint.pdf)\n",
    "2. Giles, M.B., Pierce, N.A. [An Introduction to the Adjoint Approach to Design](https://people.maths.ox.ac.uk/gilesm/files/ftc00.pdf), Flow, Turbulence and Combustion 65, 393–415 (2000)\n",
    "2. A. Ben-Tal, A. Nemirovski, [Lecture notes. Optimization III](https://www2.isye.gatech.edu/~nemirovs/OPTIIILN2024Spring.pdf)\n",
    "3. T. W. Hughes, M. Minkov, I. A. D. Williamson, and S. Fan, [Adjoint Method and Inverse Design for Nonlinear Nanophotonic Devices](http://pubs.acs.org/action/showCitFormats?doi=10.1021/acsphotonics.8b01522) ACS Photonics 2018 5 (12), 4781-4787\n",
    "4. Rasmus E. Christiansen and Ole Sigmund, [Inverse design in photonics by topology optimization: tutorial](https://opg.optica.org/josab/fulltext.cfm?uri=josab-38-2-496&id=446780) J. Opt. Soc. Am. B 38, 496-509 (2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
