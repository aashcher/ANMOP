{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3ba2b9",
   "metadata": {},
   "source": [
    "MIT Licence\n",
    "\n",
    "© Alexey A. Shcherbakov, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffec2d",
   "metadata": {},
   "source": [
    "# Lecture 5.3. Deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab869914",
   "metadata": {},
   "source": [
    "Deep learning is based on the use of deep neural networks (DNNs) to solve both forward and inverse problems. Unlike traditional analytical approaches, deep learning is useful when knowledge of the system under consideration is insufficient to apply more traditional optimization methods, or when there are too many degrees of freedom, making it difficult to predict the system's behavior. In such \"black box\" scenarios, networks can be trained using only input and output data, without the need for knowledge and understanding of the internal workings of the system. On the one hand, this approach is somewhat at odds with the traditional scientific and engineering method, but there are situations where it is the only way to obtain the required solutions. Below, we briefly review the general concepts and approaches of deep learning, followed by examples of its use in optical and photonic problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6a229",
   "metadata": {},
   "source": [
    "## Artificial neural networks\n",
    "\n",
    "The main component of neural networks is the so-called artificial neuron, which is implemented using a vector mathematical function that is a nonlinear transformation of the weighted sum of the components of the input vector, to which a nonlinear \"activation function\" $\\varphi$ is applied:\n",
    "$$ f:\\;\\boldsymbol{x}\\rightarrow a = \\boldsymbol{w}^T\\boldsymbol{x}+b \\rightarrow y=\\varphi(a) $$\n",
    "The activation function is assumed to be continuously differentiable except for a finite number of points, and is typically a sigmoid, ReLU, or hyperbolic tangent. The neuron is analogous to logistic regression.\n",
    "\n",
    "<figure> <center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/1920px-Artificial_neuron_structure.svg.png\" alt=\"Artificial Neuron, Wiki\" width=\"500\">\n",
    "<br><caption>Visualization of an artificial neuron (Wiki)</caption>\n",
    "</center> </figure>\n",
    "\n",
    "A neural network is a set of artificial neurons connected by links, organized to solve a certain problem. The network can be represented as a graph, the nodes of which are neurons, and the edges show the connections between them. The connections show how the outputs of some functions implementing neurons are connected with the inputs of other neurons. Often, people work with layers of neural networks and connections between layers. The entire set of neurons in the network, their activation functions and connections between them, as well as types of layers, is called the network architecture. They distinguish an input layer that receives the initial vector of parameters (or input signal), an output layer that produces a vector - the result of the network's work, and, possibly, additional hidden layers. If there are more than one hidden layers, the network is called deep.\n",
    "\n",
    "<figure> <center>\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240719175629/Input-Layer-in-an-ANN.webp\" alt=\"Artificial Neuron, Wiki\" width=\"500\">\n",
    "<br><caption><a href=\"https://www.geeksforgeeks.org/deep-learning/layers-in-artificial-neural-networks-ann/\">Пример нейронной сети со скрытыми слоями</a></p></caption>\n",
    "</center> </figure>\n",
    "\n",
    "An example of a \"zoo\" of neural network architectures from the website [The Asimov Institute](https://www.asimovinstitute.org) is shown in the figure below\n",
    "<figure> <center>\n",
    "<img src=\"https://www.asimovinstitute.org/wp-content/uploads/2019/04/NeuralNetworkZoo20042019.png\" alt=\"The Neural Network Zoo\" width=\"500\">\n",
    "</center> </figure>\n",
    "\n",
    "Hidden layers of neurons in the network can be of different types - fully connected, convolutional, recurrent, dropout, pooling and batch normalization layers. In a fully connected layer, each neuron is connected to all neurons in the previous and subsequent layers, and the weights of the neurons are network parameters. A convolutional layer is an application of the convolution operation to the outputs of the previous layer, so that the convolution kernel is a network parameter. Recurrent layers use feedback loops, usually with a delay. Dropout layers are used to regularize the network and avoid overfitting, so that when training the network (finding optimal parameters), neurons in such a layer are randomly excluded from work. Pooling layers are used to reduce the dimensionality of data, for example, by calculating the maximum or average value of blocks of input data. Batch normalization layers are used to speed up training, and normalize the output of the previous layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "The learning task is reduced to finding the weights and shifts of the functions of neurons, lightrock kernels, etc. Learning is based on the backpropagation algorithm - calculating the gradient for all network parameters in order to optimize the parameter values ​​on the training sample with known results of the network target function. When initializing the network, the weights characterizing the functions of neurons are set randomly. Then, in the iterative optimization algorithm, the weights are adjusted so that the input and output data of the network correspond to the values ​​in the so-called training set. This optimization problem can be formulated as a problem of minimizing a certain functional - a loss function that quantitatively determines the difference between the network output data and the specified values ​​from the training set. For example, in regression problems - predicting the output numerical values ​​of some continuous quantity given the input data - the loss function is often chosen as the standard deviation\n",
    "$$ L(y,\\hat{y}) = \\frac{1}{N} \\sum_{n=1}^{N} (y_n-\\hat{y}_n)^2 $$\n",
    "where $y_n$ is the output data of the training sample, and $\\hat{y}_n$ is the output values ​​of the current state of the network. The value of $N$ can be either equal to the entire volume of the training sample or smaller than it. In the second case, a reference set for training is randomly selected from the entire sample.\n",
    "\n",
    "Most problems in machine and deep learning, in particular, as applied to physics, are reduced to the so-called generative and discriminative models. Discriminative networks are needed to solve regression and classification problems. They interpolate relations in training data. The mapping of input parameters to output is a single-valued function, which is a mapping of a multidimensional vector space to a one-dimensional or also multidimensional one. In terms of probabilities, within the framework of discriminative modeling, for some object $x$ and characteristic $y$, the conditional probability function $p(y|x)$ is restored. Thus, discriminative neural networks can serve as surrogate physical models that solve a direct problem, for example, scattering or diffraction. Compared to rigorous numerical models of physical phenomena, a trained discriminative network can estimate the solution to a direct problem orders of magnitude faster than classical rigorous methods.\n",
    "\n",
    "Generative models are used to generate objects of a certain class according to given characteristics. In generative modeling, the probability $p(x)$ or $p(x|y)$ is restored. As a result of training a generative model, a certain function is obtained that produces objects of a given class. Examples of generative models are generative adversarial neural networks (GAN), diffusion models, and variational autoencoders. In these models, an object is a function of a random vector whose components have a simple probability distribution (e.g., uniform or normal) $x=G(z)$, where $G$ is the generator function, and $z$ is the vector of the so-called latent space. Specifying directions in the latent space allows smoothly changing various characteristics of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbaf04",
   "metadata": {},
   "source": [
    "## Discriminative modeling of optical and photonic devices\n",
    "\n",
    "The rapid development of artificial intelligence methods, which began in the late 2000s in connection with the development of video card production technologies, quickly led to the active use of these methods in natural science problems, in particular in optics and photonics. The figure below shows some examples of applications of various types of neural networks for surrogate modeling in solving direct problems of optics and photonics. To train the networks, data sets of tens of thousands of samples were generated using rigorous modeling.\n",
    "\n",
    "<figure> <center>\n",
    "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41578-020-00260-1/MediaObjects/41578_2020_260_Fig3_HTML.png?as=webp\" width=\"1000\">\n",
    "<br><caption><a href=\"https://www.nature.com/articles/s41578-020-00260-1\">Examples of surrogate models in optics and photonics. (a) Timeline of the emergence of various applications, (b) Approximation of scattering spectra of multilayer spherical particles using a fully connected network, (c) Calculation of spatial polarization of a nanostructure excited by an electromagnetic wave using a convolutional network, (d) Model for calculating the spectrum of coupled ring resonators using a graph network</a></p></caption>\n",
    "</center> </figure>\n",
    "\n",
    "Several different approaches using neural networks have been proposed to solve inverse problems. One possibility is to use the backpropagation algorithm to correct input data, which, for example, define the geometry of the optical device being calculated, for a trained neural network with fixed weights.\n",
    "\n",
    "<figure> <center>\n",
    "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41578-020-00260-1/MediaObjects/41578_2020_260_Fig4_HTML.png?as=webp\" width=\"1000\">\n",
    "<br><caption><a href=\"https://www.nature.com/articles/s41578-020-00260-1\">Examples of solving the inverse problem using neural networks. (a,b) application of a pre-trained network and the backpropagation algorithm, (c) an example of using a direct surrogate model in conjunction with a genetic algorithm, (d-e) application of a surrogate direct model for training a network to solve the inverse problem</a></p></caption>\n",
    "</center> </figure>\n",
    "\n",
    "Another approach is to use discriminative models as direct solvers in combination with classical optimization algorithms, some of which were mentioned in previous lectures. An example of such an approach is shown in the figure above. It was also suggested to use combinations of neural networks to solve first the direct problem and then the inverse problem in cases where the use of strict direct and classical optimization methods for solving inverse problems turns out to be excessively labor-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2b752",
   "metadata": {},
   "source": [
    "## Generative modeling of optical and photonic devices\n",
    "\n",
    "The properties of generative models allow them to be used in different ways in optical device design. The first way is to train an unconditional network on a set of devices that represent a small sample of the target subset of the design space. Training sets can represent devices of different geometries that collectively have a set of desired optical responses. In this case, the trained network will generate distributions of devices that more completely fill the design subspace. Training sets can also include variations of the same device type, so that the trained network will generate more geometric variations of that device type, some of which may perform better than the devices in the training set.\n",
    "\n",
    "The second strategy is to train a conditional network on sets of high-performance optimized devices. If the training set consists of devices operating with certain discrete conditional label values, the trained network will be able to generalize and generate device prototypes across a continuous range of label values. This ability to generate devices with conditional label values ​​interpolated from the training set is analogous to regression with discriminative networks.\n",
    "\n",
    "A third strategy is to initially train either a conditional or unconditional generative network and then use conventional optimization methods on the latent parameter space. This has parallels with the approach of using a discriminative model as a surrogate electromagnetic solver in combination with conventional optimization methods. The key difference here is that generative networks provide greater control over the search space of potential optimal devices.\n",
    "\n",
    "<figure> <center>\n",
    "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41578-020-00260-1/MediaObjects/41578_2020_260_Fig6_HTML.png?as=webp\" width=\"1000\">\n",
    "<br><caption><a href=\"https://www.nature.com/articles/s41578-020-00260-1\">An illustration of how generative AI can be applied to solving inverse problems in optics and photonics: (a) an unconditional network is trained on a space of devices and used to find similar devices; (b) a conditional network is used to generate a distribution of devices using conditional parameter labels interpolated from the training set - for example, the network learns devices operating at 800 and 1000 nm wavelengths and can interpolate the distribution of devices operating at 900 nm; (c) using classical optimization methods to find the optimal space on a set of latent parameters.</a></p></caption>\n",
    "</center> </figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb97f5a",
   "metadata": {},
   "source": [
    "## Physics-informed neural networks\n",
    "\n",
    "Physics-inspired neural networks are a class of machine learning models that integrate physical laws (expressed as differential equations) into the training of the neural network. Unlike traditional neural networks that rely solely on data, PINNs use known physics to improve, generalize, reduce data dependence, and provide physically consistent predictions. The main components of PINN models are:\n",
    "- a neural network architecture - often a deep neural network that approximates a solution to a physical problem.\n",
    "- a physics-inspired loss function that, in addition to the usual terms, may include residuals from equations describing physical processes (e.g., Maxwell's equations, the wave equation) or conservation laws.\n",
    "- automatic differentiation, used to compute derivatives of the network's output with respect to the input data, enforcing physical constraints without computing finite differences.\n",
    "- hybrid learning, which combines supervised learning (from sparse data) and unsupervised learning (from physical equations).\n",
    "\n",
    "For example, if we solve a partial differential equation $\\mathcal{L}(u) = f, \\boldsymbol{x}\\in\\Omega$ with boundary conditions $\\mathcal{B}(u)=g, \\boldsymbol{x}\\in\\partial\\Omega$, a typical loss function for PINN looks like\n",
    "$$ L = L_{d} + \\alpha L_{p} $$\n",
    "where the first term is due to the data $L_d = || u_{pred} - u_{obs} ||$, and the second term is due to residuals associated with the physical problem\n",
    "$$ L_p = ||\\mathcal{L}(u_{pred}) - f|| + || \\mathcal{B}(u_{pred})-g || $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862fbda",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "1. Jiang, J., Chen, M. & Fan, J.A. [Deep neural networks for the evaluation and design of photonic devices](https://www.nature.com/articles/s41578-020-00260-1), Nat. Rev. Mater. 6, 679–700 (2021)\n",
    "2. Pedro Freire, Egor Manuylovich, Jaroslaw E. Prilepsky, and Sergei K. Turitsyn, [Artificial neural networks for photonic applications—from algorithms to implementation: tutorial](https://opg.optica.org/aop/fulltext.cfm?uri=aop-15-3-739&id=539680), Adv. Opt. Photon. 15, 739-834 (2023)\n",
    "3. Cuomo, S., Di Cola, V.S., Giampaolo, F. et al. [Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](link.springer.com/article/10.1007/S10915-022-01939-Z). J Sci Comput 92, 88 (2022)\n",
    "4. Vlad Medvedev, Andreas Erdmann, and Andreas Rosskopf, [Physics-informed deep learning for 3D modeling of light diffraction from optical metasurfaces](https://opg.optica.org/oe/fulltext.cfm?uri=oe-33-1-1371&id=566869), Opt. Express 33, 1371-1384 (2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
